{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlantVillage: DL vs classical.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "seQu6b_91C4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "import tensorflow as tf\n",
        "from skimage import color\n",
        "import scipy.stats as stats\n",
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "from sklearn.neural_network import MLPClassifier as FCNN\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xnRTYHh2l7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the dataset\n",
        "os.system(\"git clone https://github.com/spMohanty/PlantVillage-Dataset.git\")\n",
        "\n",
        "# Get positioned in the PlantVillage directory\n",
        "os.chdir(\"/content/PlantVillage-Dataset/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHjvnXq1C7PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define global constants\n",
        "batch_size=24\n",
        "TRAIN_PERCENT = 0.8\n",
        "\n",
        "IMAGE_TYPE='segmented'\n",
        "\n",
        "INPUT_FOLDER = \"./raw/\" + IMAGE_TYPE\n",
        "OUTPUT_FOLDER = \"./lmdb\"\n",
        "\n",
        "train_dir='raw/' + IMAGE_TYPE + '/'\n",
        "val_dir='raw/' + IMAGE_TYPE + '_test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6_aSsnPC68s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code in this cell was taken from https://github.com/spMohanty/PlantVillage-Dataset.git repository, refactored and adjusted to work with python3\n",
        "# Define functions for splitting data into training and test sets\n",
        "\n",
        "leaf_map = json.loads(open(\"leaf-map.json\", \"r\").read())\n",
        "\n",
        "def determine_leaf_group(leaf_identifier, class_name):\n",
        "\tglobal leaf_map\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tfoo = leaf_map[leaf_identifier.lower().strip()]\n",
        "\t\tif len(foo) == 1:\n",
        "\t\t\treturn foo[0]\n",
        "\t\telse:\n",
        "\t\t\tfor _suggestion in foo:\n",
        "\t\t\t\tif _suggestion.find(class_name) != -1:\n",
        "\t\t\t\t\treturn _suggestion\n",
        "\t\t\treturn str(random.randint(1,10000000000000000000000))\n",
        "\texcept:\n",
        "\t\treturn str(random.randint(1,10000000000000000000000))\n",
        "\n",
        "def compute_per_class_distribution(DATASET):\n",
        "\tclass_map = {}\n",
        "\tcount = 0\n",
        "\tfor datum in DATASET:\n",
        "\t\ttry:\n",
        "\t\t\tclass_map[datum[1]].append(datum[0])\n",
        "\t\t\tcount += 1\n",
        "\t\texcept:\n",
        "\t\t\tclass_map[datum[1]] = [datum[0]]\n",
        "\t\t\tcount += 1\n",
        "\tfor _key in class_map:\n",
        "\t\tclass_map[_key] = len(class_map[_key]) \n",
        "\n",
        "\treturn class_map\n",
        "\n",
        "def distribute_buckets(BUCKETS, train_probability):\n",
        "\ttrain = []\n",
        "\ttest = []\n",
        "\t\n",
        "\tfor _key in BUCKETS.keys():\n",
        "\t\tbucket = BUCKETS[_key]\n",
        "\n",
        "\t\tif random.random() <= train_probability:\n",
        "\t\t\ttrain += bucket\n",
        "\t\telse:\n",
        "\t\t\ttest += bucket\t\n",
        "\treturn train, test\t\n",
        "\n",
        "def get_target_folder_name(train_prob):\n",
        "   return str(int(math.ceil(train_prob*100)))+\"-\"+str(int(math.ceil((1-train_prob)*100)))\n",
        "\n",
        "def create_dataset_maps(INPUT_FOLDER=INPUT_FOLDER, OUTPUT_FOLDER=OUTPUT_FOLDER, train_prob=TRAIN_PERCENT):\n",
        "  BUCKETS = {}\n",
        "  all_images = glob.glob(INPUT_FOLDER+\"/*/*\")\n",
        "  for _img in all_images:\n",
        "    image_name = _img.split(\"/\")[-1]\n",
        "    class_name = _img.split(\"/\")[-2]\n",
        "    #Check if the image belongs to a particular known group\n",
        "    image_identifier = image_name.replace(\"_final_masked\",\"\")\n",
        "    image_identifier = image_identifier.split(\"___\")[-1]\t\n",
        "    image_identifier = image_identifier.split(\"copy\")[0].replace(\".jpg\", \"\").replace(\".JPG\",\"\").replace(\".png\",\"\").replace(\".PNG\", \"\")\n",
        "\n",
        "    group = determine_leaf_group(image_identifier, class_name)\n",
        "    try:\n",
        "      BUCKETS[group].append((_img, class_name))\n",
        "    except:\n",
        "      BUCKETS[group] = [(_img, class_name)]\n",
        "\n",
        "  CANDIDATE_DISTRIBUTIONS = []\n",
        "  CANDIDATE_VARIANCES = []\n",
        "  for k in range(1000):\n",
        "    train, test = distribute_buckets(BUCKETS, train_prob)\n",
        "    train_dist = compute_per_class_distribution(train) \n",
        "    test_dist =  compute_per_class_distribution(test) \n",
        "    spread_data = []\n",
        "    for _key in train_dist:\n",
        "      spread_data.append(train_dist[_key] * 1.0 /(train_dist[_key]+test_dist[_key]))\n",
        "\n",
        "    CANDIDATE_DISTRIBUTIONS.append((train, test))\n",
        "    CANDIDATE_VARIANCES.append(np.var(spread_data))\n",
        "\n",
        "  train, test = CANDIDATE_DISTRIBUTIONS[np.argmax(CANDIDATE_VARIANCES)]\n",
        "  print(len(train))\n",
        "  print(len(test))\n",
        "\n",
        "  train_dist = compute_per_class_distribution(train)\n",
        "  test_dist =  compute_per_class_distribution(test)\n",
        "  spread_data = []\n",
        "  for _key in train_dist:\n",
        "    print(_key, train_dist[_key] * 1.0 /(train_dist[_key]+test_dist[_key]))\n",
        "    spread_data.append(train_dist[_key] * 1.0 /(train_dist[_key]+test_dist[_key]))\n",
        "\n",
        "  print(\"Mean :: \", np.mean(spread_data))\n",
        "  print(\"Variance: \", np.var(spread_data))\n",
        "\n",
        "  target_folder_name = get_target_folder_name(train_prob)\n",
        " \n",
        "  try:\n",
        "    os.makedirs(OUTPUT_FOLDER+\"/\"+target_folder_name)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  labels_map = {}\n",
        "  for _entry in train:\n",
        "    try:\n",
        "      labels_map[_entry[1]] += 1\n",
        "    except:\n",
        "      labels_map[_entry[1]] = 1\n",
        "  print(labels_map)\n",
        "  labels_list = sorted(labels_map.keys())\n",
        "\n",
        "  traintxt = OUTPUT_FOLDER+\"/\"+target_folder_name+\"/train.txt\";\n",
        "  f = open(traintxt,\"w\")\n",
        "  train_txt = \"\"\n",
        "  for _entry in train:\n",
        "    train_txt += os.path.abspath(_entry[0])+\"\\t\"+str(labels_list.index(_entry[1]))+\"\\n\"\n",
        "  f.write(train_txt)\n",
        "  f.close()\n",
        "\n",
        "  testtxt = OUTPUT_FOLDER+\"/\"+target_folder_name+\"/test.txt\"\n",
        "  f = open(testtxt,\"w\")\n",
        "  test_txt = \"\"\n",
        "  for _entry in test:\n",
        "    test_txt += os.path.abspath(_entry[0])+\"\\t\"+str(labels_list.index(_entry[1]))+\"\\n\"\n",
        "  f.write(test_txt)\n",
        "  f.close()\n",
        "\n",
        "  labelstxt = OUTPUT_FOLDER+\"/\"+target_folder_name+\"/labels.txt\"\n",
        "  f = open(labelstxt,\"w\")\n",
        "  f.write(\"\\n\".join(labels_list))\n",
        "  f.close()\n",
        "\n",
        "  return traintxt, testtxt, labelstxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbjXJKqDwYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for moving training and testing samples into separate directories\n",
        "\n",
        "def create_datasets(INPUT_FOLDER=INPUT_FOLDER, OUTPUT_FOLDER=OUTPUT_FOLDER, train_prob=TRAIN_PERCENT):\n",
        "\n",
        "  for i in range(1,10):\n",
        "    try:\n",
        "      _, testtxt, labelstxt = create_dataset_maps(INPUT_FOLDER, OUTPUT_FOLDER, train_prob)\n",
        "      break\n",
        "    except Exception as e:\n",
        "      print('Creating datasets failed ' + str(i) +', trying again...: ' + str(e))\n",
        "\n",
        "  with open(labelstxt) as file:\n",
        "    class_name = file.readline().strip()\n",
        "    while class_name:\n",
        "      os.makedirs(val_dir + class_name)\n",
        "      class_name = file.readline().strip()\n",
        "\n",
        "  with open(testtxt) as file:\n",
        "    old_path = file.readline().rstrip().split('\\t')[0]\n",
        "    while old_path:\n",
        "      new_path = old_path.replace(IMAGE_TYPE, IMAGE_TYPE + '_test')\n",
        "      try:\n",
        "        os.rename(old_path, new_path)\n",
        "      except:\n",
        "        print(\"Error: \" + old_path)\n",
        "      \n",
        "      old_path = file.readline().rstrip().split('\\t')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9j7Ii-814Uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DL functions\n",
        "\n",
        "def learning_rate_decay(epoch, learning_rate):\n",
        "  return 0.005 /(10**(epoch//10))\n",
        "\n",
        "def preprocessing_per_image(image):\n",
        "  image /= 255;\n",
        "  image -= 0.5;\n",
        "  image *= 2;\n",
        "  return image;\n",
        "\n",
        "def construct_model():\n",
        "  base_model = tf.keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(256,256,3))  \n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(base_model)\n",
        "  model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "  model.add(tf.keras.layers.Dense(38, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(\n",
        "      decay= 0.0005,\n",
        "      momentum=0.9\n",
        "  ),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['acc']\n",
        "  )\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def train_dl_model():\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( \n",
        "    preprocessing_function = preprocessing_per_image,\n",
        "  )\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          train_dir,\n",
        "          target_size=(256, 256),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "\n",
        "  val_generator = train_datagen.flow_from_directory(\n",
        "          val_dir,\n",
        "          target_size=(256, 256),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "\n",
        "  model = construct_model()\n",
        "\n",
        "  lrate = tf.keras.callbacks.LearningRateScheduler(learning_rate_decay)\n",
        "\n",
        "  history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
        "        epochs=10,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples/val_generator.batch_size,\n",
        "        verbose=1,\n",
        "        callbacks=[lrate]    \n",
        "  )\n",
        "\n",
        "  return model, history\n",
        "\n",
        "def test_dl_model(model):\n",
        "  datagen = tf.keras.preprocessing.image.ImageDataGenerator( \n",
        "    preprocessing_function = preprocessing_per_image,\n",
        "  )\n",
        "\n",
        "  test_generator = datagen.flow_from_directory(\n",
        "          val_dir,\n",
        "          target_size=(256, 256),\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical',\n",
        "          shuffle=False)\n",
        "\n",
        "  predictions = np.argmax(model.predict_generator(test_generator), axis=1)\n",
        "  labels = test_generator.classes\n",
        "  print(\"DL test metrics:\" + str(prediction_metrics(labels, predictions)))\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVpXXSraSYrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature extraction functions\n",
        "\n",
        "def remove_green_pixels(image):\n",
        "  # Transform from (256,256,3) to (3,256,256)\n",
        "  channels_first = channels_first_transform(image)\n",
        "\n",
        "  r_channel = channels_first[0]\n",
        "  g_channel = channels_first[1]\n",
        "  b_channel = channels_first[2]\n",
        "\n",
        "  # Set those pixels where green value is larger than both blue and red to 0\n",
        "  mask = False == np.multiply(g_channel > r_channel, g_channel > b_channel)\n",
        "  channels_first = np.multiply(channels_first, mask)\n",
        "\n",
        "  # Transfrom from (3,256,256) back to (256,256,3)\n",
        "  image = channels_first.transpose(1, 2, 0)\n",
        "  return image\n",
        "\n",
        "def rgb2lab(image):\n",
        "  return color.rgb2lab(image)\n",
        "\n",
        "def rgb2gray(image):\n",
        "  return np.array(color.rgb2gray(image) * 255, dtype=np.uint8)\n",
        "\n",
        "def glcm(image, offsets=[1], angles=[0], squeeze=False):\n",
        "  single_channel_image = image if len(image.shape) == 2 else rgb2gray(image)\n",
        "  gclm = greycomatrix(single_channel_image, offsets, angles)\n",
        "  return np.squeeze(gclm) if squeeze else gclm\n",
        "\n",
        "def histogram_features_bucket_count(image):\n",
        "  image = channels_first_transform(image).reshape(3,-1)\n",
        "\n",
        "  r_channel = image[0]\n",
        "  g_channel = image[1]\n",
        "  b_channel = image[2]\n",
        "\n",
        "  r_hist = np.histogram(r_channel, bins = 26, range=(0,255))[0]\n",
        "  g_hist = np.histogram(g_channel, bins = 26, range=(0,255))[0]\n",
        "  b_hist = np.histogram(b_channel, bins = 26, range=(0,255))[0]\n",
        "\n",
        "  return np.concatenate((r_hist, g_hist, b_hist))\n",
        "\n",
        "def histogram_features(image):\n",
        "  color_histogram = np.histogram(image.flatten(), bins = 255, range=(0,255))[0]\n",
        "  return np.array([\n",
        "    np.mean(color_histogram),\n",
        "    np.std(color_histogram),\n",
        "    stats.entropy(color_histogram),\n",
        "    stats.kurtosis(color_histogram),\n",
        "    stats.skew(color_histogram),\n",
        "    np.sqrt(np.mean(np.square(color_histogram)))\n",
        "  ])\n",
        "\n",
        "def texture_features(full_image, offsets=[1], angles=[0], remove_green = True):\n",
        "  image = remove_green_pixels(full_image) if remove_green else full_image\n",
        "  gray_image = rgb2gray(image)\n",
        "  glcmatrix = glcm(gray_image, offsets=offsets, angles=angles)\n",
        "  return glcm_features(glcmatrix)\n",
        "\n",
        "def glcm_features(glcm):\n",
        "  return np.array([\n",
        "    greycoprops(glcm, 'correlation'),\n",
        "    greycoprops(glcm, 'contrast'),\n",
        "    greycoprops(glcm, 'energy'),\n",
        "    greycoprops(glcm, 'homogeneity'),\n",
        "    greycoprops(glcm, 'dissimilarity'),\n",
        "  ]).flatten()\n",
        "\n",
        "def channels_first_transform(image):\n",
        "  return image.transpose((2,0,1))\n",
        "\n",
        "def extract_features(image):\n",
        "  offsets=[1,3,10,20]\n",
        "  angles=[0, np.pi/4, np.pi/2]\n",
        "  channels_first = channels_first_transform(image)\n",
        "  return np.concatenate((\n",
        "      texture_features(image, offsets=offsets, angles=angles),\n",
        "      texture_features(image, offsets=offsets, angles=angles, remove_green=False),\n",
        "      histogram_features_bucket_count(image),\n",
        "      histogram_features(channels_first[0]),\n",
        "      histogram_features(channels_first[1]),\n",
        "      histogram_features(channels_first[2]),\n",
        "      ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cZAcXYlD1iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM functions\n",
        "\n",
        "def map_class_to_label(dir_path):\n",
        "  class_paths = glob.glob(dir_path+'*')\n",
        "  class_to_label = {}\n",
        "  i = 0\n",
        "  for full_path in class_paths:\n",
        "    class_to_label[full_path.split('/')[-1]] = i\n",
        "    i+=1\n",
        "  return class_to_label\n",
        "\n",
        "def get_classes(dir_path):\n",
        "  return glob.glob(dir_path + '/*')\n",
        "\n",
        "def load_svm_set_and_labels(dir_path, class_to_label, max_examples_per_class=100000):\n",
        "  classes = get_classes(dir_path)\n",
        "  \n",
        "  shape = (256,256,3)\n",
        "\n",
        "  tset = []\n",
        "  labels = []\n",
        "\n",
        "  for data_class in classes:\n",
        "    examples_count = 0;\n",
        "    label = int(class_to_label[data_class.split('/')[-1]])\n",
        "    images = glob.glob(data_class + '/*')\n",
        "    random.shuffle(images)\n",
        "    for image_path in images:\n",
        "      image = img.imread(image_path)\n",
        "      if (image.shape!= shape):\n",
        "        print(f\"Found wrong image shape: {image_path}. Shape {image.shape} instead of {shape}\")\n",
        "        continue\n",
        "      features = extract_features(image)\n",
        "      tset.append(features)\n",
        "      labels.append(label)\n",
        "      examples_count+=1\n",
        "      if (examples_count >= max_examples_per_class):\n",
        "        break\n",
        "\n",
        "    print('Finished feature extraction for class ' + data_class)\n",
        "  return tset, np.array(labels, dtype=np.uint8)\n",
        "\n",
        "def load_svm_prepared_dataset():\n",
        "  train_dir = 'data_distribution_for_SVM/train/';\n",
        "  test_dir = 'data_distribution_for_SVM/test/';\n",
        "  class_to_label = map_class_to_label(train_dir)\n",
        "\n",
        "  training_set, labels = load_svm_set_and_labels(train_dir, class_to_label)\n",
        "  test_set, test_labels = load_svm_set_and_labels(test_dir, class_to_label)\n",
        "  \n",
        "  return training_set, labels, test_set, test_labels\n",
        "\n",
        "def load_svm_dataset(max_examples_per_class=100000):\n",
        "  class_to_label = map_class_to_label(train_dir)\n",
        "  training_set, training_labels = load_svm_set_and_labels(train_dir, class_to_label, max_examples_per_class*TRAIN_PERCENT)\n",
        "  test_set, test_labels = load_svm_set_and_labels(val_dir, class_to_label, max_examples_per_class*(1-TRAIN_PERCENT))\n",
        "  return training_set, training_labels, test_set, test_labels\n",
        "\n",
        "def normalize_features(training_features, test_features):\n",
        "  training_mean = np.mean(training_features, 0)\n",
        "  trainingStd = np.std(training_features, 0) + 0.0001;\n",
        "\n",
        "  training_features_normalized = (training_features - training_mean)/trainingStd\n",
        "  test_features_normalized = (test_features - training_mean)/trainingStd\n",
        "\n",
        "  return training_features_normalized, test_features_normalized\n",
        "\n",
        "def format_labels(labels):\n",
        "  return np.array(labels, dtype=np.int32)\n",
        "\n",
        "def shuffle_data(training_set, labels):\n",
        "  set_size = len(training_set)\n",
        "  indexes = random.sample(list(range(set_size)),set_size)\n",
        "  training_set = training_set[indexes]\n",
        "  labels = labels[indexes]\n",
        "\n",
        "  return training_set, labels\n",
        "\n",
        "def construct_svm_classifier(kernel='linear', C=10, max_iter=10000):\n",
        "  if kernel=='linear':\n",
        "    return svm.LinearSVC(C=C,verbose=1, max_iter=max_iter)\n",
        "  else:\n",
        "    return svm.SVC(C=C,kernel=kernel, verbose=1, max_iter=max_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVTetJLmUDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test\n",
        "\n",
        "def prepare_dataset():\n",
        "  # Load and format the datasets so they can be used with scikit-learn classifiers\n",
        "  start = time.time()\n",
        "  training_set, training_labels, test_set, test_labels = load_svm_dataset()\n",
        "  training_set, test_set = normalize_features(training_set, test_set)\n",
        "  training_set, training_labels = shuffle_data(training_set, training_labels)\n",
        "  print('Elapsed time: '+ str((time.time()-start)) + ' seconds')\n",
        "  return training_set, training_labels, test_set, test_labels\n",
        "\n",
        "def train(classifier, training_set, training_labels):\n",
        "  start = time.time()\n",
        "  classifier.fit(training_set, training_labels)\n",
        "  print('Elapsed time: '+ str((time.time()-start)) + ' seconds')\n",
        "  return classifier\n",
        "\n",
        "def test(classifier, dataset, labels):\n",
        "  start = time.time()\n",
        "  predict = classifier.predict(dataset)\n",
        "  print(\"Prediction metrics: \" + str(prediction_metrics(labels, predict)))\n",
        "  per_class_hits(labels, predict)\n",
        "  print('Elapsed time: '+ str(time.time()-start) + ' seconds')\n",
        "\n",
        "def train_and_test(classifier, training_set, training_labels, test_set, test_labels):\n",
        "  classifier = train(classifier, training_set, training_labels);\n",
        "  print('Training metrics:')\n",
        "  test(classifier, training_set, training_labels)\n",
        "  print('Testing metrics:')\n",
        "  test(classifier, test_set, test_labels)\n",
        "  return classifier\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo0lESvYpmAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define models\n",
        "\n",
        "def knn_model():\n",
        "  return KNN(n_neighbors=5)\n",
        "\n",
        "def svm_model():\n",
        "  return construct_svm_classifier(kernel='rbf', C=100, max_iter=10000000)\n",
        "\n",
        "def fcnn_model():\n",
        "  return FCNN(solver='adam', alpha=3e-1,hidden_layer_sizes=(300, 200, 100, 50), random_state=1, max_iter=2000, activation='relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhk4c93sf248",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing functions\n",
        "\n",
        "def plot_image(image_path=None, image_folder = 'segmented'):\n",
        "  if image_path == None:\n",
        "    image_path = random.choice(glob.glob(f\"raw/{image_folder}/*/*\"))\n",
        "  \n",
        "  image = img.imread(image_path)\n",
        "  print(image_path)\n",
        "  figure = plt.figure(figsize=(12,12))\n",
        "  figure.add_subplot(1, 4, 1)\n",
        "  plt.imshow(image)\n",
        "  figure.add_subplot(1, 4, 2)\n",
        "  plt.imshow(np.log(glcm(image, squeeze=True) + 1)**0.5, cmap='gray')\n",
        "  figure.add_subplot(1, 4, 3)\n",
        "  plt.imshow(remove_green_pixels(image))\n",
        "  figure.add_subplot(1, 4, 4)\n",
        "  plt.imshow(np.log(glcm(remove_green_pixels(image), squeeze=True) + 1)**0.5, cmap='gray')\n",
        "\n",
        "def prediction_metrics(true_values, predicted_values, average='macro'):\n",
        "  return {\n",
        "      'accuracy': round(metrics.accuracy_score(true_values, predicted_values), 3),\n",
        "      'f1': round(metrics.f1_score(true_values, predicted_values, average=average), 3),\n",
        "      'precision': round(metrics.precision_score(true_values, predicted_values, average=average), 3),\n",
        "      'recall': round(metrics.recall_score(true_values, predicted_values, average=average), 3),\n",
        "  }\n",
        "\n",
        "def per_class_hits(labels, predictions):\n",
        "  results = {}\n",
        "  for i in set(labels):\n",
        "    results[i] = {'hits':0, 'misses':0}\n",
        "\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == predictions[i]:\n",
        "      results[labels[i]]['hits']+=1\n",
        "    else:\n",
        "      results[labels[i]]['misses']+=1\n",
        "\n",
        "  for i in results.keys():\n",
        "    results[i]['percent'] = round(results[i]['hits'] / (results[i]['hits'] + results[i]['misses']),3)\n",
        "    print({i:results[i]})\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14gG4_2Q14vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets\n",
        "create_datasets(INPUT_FOLDER, OUTPUT_FOLDER, TRAIN_PERCENT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-oeZIXW15N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test the deep learning model (GoogLeNet)\n",
        "model, history = train_dl_model()\n",
        "test_dl_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QnRkrUV15gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test classical models\n",
        "training_set, training_labels, test_set, test_labels = prepare_dataset()\n",
        "\n",
        "train_and_test(knn_model(), training_set, training_labels, test_set, test_labels)\n",
        "train_and_test(svm_model(), training_set, training_labels, test_set, test_labels)\n",
        "train_and_test(fcnn_model(), training_set, training_labels, test_set, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
